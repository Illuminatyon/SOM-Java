# Self Organizing Map

![Licencia: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Universidad: Paris 8](https://img.shields.io/badge/Universidad-Paris%208-blue)
![Machine: learning](https://img.shields.io/badge/Machine-Learning-orange)
![Java: 17](https://img.shields.io/badge/Java-17-red)
![Colaboradores](https://img.shields.io/badge/Colaboradores-1-brightgreen)
![Estrellas](https://img.shields.io/badge/Estrellas-0-lightgrey)
![Bifurcaciones](https://img.shields.io/badge/Bifurcaciones-0-lightgrey)
![Observadores](https://img.shields.io/badge/Observadores-0-lightgrey)

## üåç Versiones multiling√ºes del README

- üá´üá∑ [Fran√ßais](README.fr.md)
- üá¨üáß [English](README.md)
- üá™üá∏ Espa√±ol (est√°s aqu√≠)

## üìò Descripci√≥n general del proyecto

Este proyecto es una implementaci√≥n simplificada de un Mapa Auto-Organizado (SOM) desarrollado por mi pasi√≥n por las matem√°ticas y mi inter√©s en ingresar a una prestigiosa escuela de ingenier√≠a. El objetivo principal era comprender el funcionamiento del algoritmo SOM recreando una versi√≥n desde cero con las siguientes restricciones:

- No se utilizan variables globales.
- Las BMUs (Best Matching Units) se almacenan en una lista enlazada.
- El tama√±o de las colecciones: matrices y listas enlazadas deben calcularse din√°micamente.

## üìä Conjunto de datos

El conjunto de datos utilizado es el conjunto de datos Iris, disponible en Kaggle:
[Conjunto de datos Iris](https://www.kaggle.com/datasets/uciml/iris)

- 150 muestras
- 4 caracter√≠sticas por muestra:
  - Longitud del s√©palo
  - Ancho del s√©palo
  - Longitud del p√©talo
  - Ancho del p√©talo
- 3 clases:
  - Iris-setosa
  - Iris-versicolor
  - Iris-virginica

## ‚öôÔ∏è Funcionamiento del algoritmo SOM

El Mapa Auto-Organizado (SOM) es una red neuronal no supervisada utilizada para la reducci√≥n de dimensionalidad y la visualizaci√≥n de datos. Estos son los pasos principales:

1. **Inicializaci√≥n**
   Las neuronas en el mapa se inicializan aleatoriamente en el espacio de caracter√≠sticas.

2. **C√°lculo de distancia**
   Para cada dato de entrada, se calcula la distancia euclidiana entre este dato y todas las neuronas.
   La neurona con la menor distancia se denomina BMU (Best Matching Unit). Este proceso sigue el principio de Winner-Takes-All (WTA), donde solo se selecciona la neurona m√°s cercana al dato de entrada, junto con sus vecinos.

3. **Actualizaci√≥n del mapa**
   La BMU y sus vecinos se ajustan para acercarse al dato de entrada, seg√∫n una tasa de aprendizaje Œ±. Esto ajusta gradualmente el mapa para representar mejor los datos de entrada.

4. **Iteraci√≥n**
   Los pasos 2 a 3 se repiten durante un n√∫mero determinado de iteraciones.
   - La tasa de aprendizaje disminuye con el tiempo.
   - El tama√±o del vecindario se reduce gradualmente, permitiendo que el mapa se especialice mientras mantiene la consistencia topol√≥gica.

El resultado es un mapa organizado topol√≥gicamente, donde clases similares se encuentran en √°reas cercanas. El algoritmo SOM agrupa as√≠ datos similares mientras preserva su estructura.

## üßë‚Äçüíª Tecnolog√≠as utilizadas

- Lenguaje: Java (implementaci√≥n desde cero)

## üíª Instalar Java (si no tienes Java instalado)

Si no tienes Java instalado, puedes seguir las instrucciones en uno de mis videos de YouTube para instalar Java en diferentes plataformas:

- Linux: [Instalar Java en Linux](https://www.youtube.com/watch?v=QauitHvQZHA)
- Mac: [Instalar Java en Mac](https://www.youtube.com/watch?v=4WKo13f2Qpc)
- Windows: [Instalar Java en Windows](https://www.youtube.com/watch?v=pShVlXCM75I)

## üìù Compilaci√≥n y ejecuci√≥n

### Clonar el repositorio

```bash
git clone https://github.com/Fab16BSB/SOM_JAVA.git
cd SOM_JAVA
```

### Compilaci√≥n

```bash
javac -d bin src/main/java/com/fabio/som/*.java
```

### Ejecuci√≥n

```bash
java -cp bin com.fabio.som.SOMApplication data/test_data.csv
```

## üìà Resultados

Cuando ejecutes la aplicaci√≥n, ver√°s el proceso de entrenamiento del SOM y la clasificaci√≥n resultante del conjunto de datos Iris. La salida mostrar√°:

1. Carga y normalizaci√≥n de datos
2. C√°lculo del vector medio
3. Creaci√≥n de vectores de peso aleatorios
4. Generaci√≥n de la cuadr√≠cula de neuronas
5. Entrenamiento SOM
6. Asignaci√≥n y visualizaci√≥n de etiquetas de clase

La salida final mostrar√° una cuadr√≠cula de neuronas con sus clases asignadas, mostrando c√≥mo el SOM ha organizado los datos en clusters.

## Componentes

La implementaci√≥n consta de cuatro clases principales:

1. **DataPoint**: Representa un punto de datos multidimensional con caracter√≠sticas num√©ricas y una etiqueta opcional.
2. **Neuron**: Representa un nodo en la cuadr√≠cula SOM, que contiene un vector de peso y una posici√≥n.
3. **SOMProcessor**: Motor de procesamiento principal que maneja la normalizaci√≥n de datos, el entrenamiento SOM y la clasificaci√≥n.
4. **SOMApplication**: Clase de aplicaci√≥n principal que proporciona una interfaz de l√≠nea de comandos.

## Detalles de implementaci√≥n

### Algoritmo de entrenamiento SOM

El proceso de entrenamiento sigue estos pasos:

1. Inicializar una cuadr√≠cula de neuronas con vectores de peso aleatorios
2. Para cada vector de entrada:
   - Encontrar la Best Matching Unit (BMU) - la neurona con el vector de peso m√°s cercano
   - Actualizar los vectores de peso de la BMU y sus vecinos para acercarlos al vector de entrada
3. Reducir gradualmente la tasa de aprendizaje y el tama√±o del vecindario
4. Asignar etiquetas de clase a las neuronas bas√°ndose en los vectores de entrada m√°s cercanos

### Par√°metros

- **L√≠mites superior/inferior**: Controlan el rango para la inicializaci√≥n aleatoria de pesos
- **Modo de entrenamiento**: Secuencial (0) o Aleatorio (1)
- **Tasa de aprendizaje**: Comienza en 0.7 y disminuye a 0.07 durante el entrenamiento
- **Radio de vecindario**: Comienza en 3 y disminuye a 1 durante el entrenamiento

## Referencias

- Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43(1), 59-69.
- Kohonen, T. (1990). The self-organizing map. Proceedings of the IEEE, 78(9), 1464-1480.
